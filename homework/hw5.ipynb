{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "## ASTR 5900, Fall 2017, University of Oklahoma\n",
    "\n",
    "### Bayesian Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "Here you are to have a qualitative and quantitative discussion on the differences between frequentist and Bayesian statistics.\n",
    "\n",
    "In the year 3001, students at the University of Mars are projecting their final grades in a machine learning course.  Students in this machine learning course are examined every Earth week, and five weeks have passed in the semester.  One such student, Calculon, has received scores of 92, 84, 95, 100, and 93 thus far.  Calculon believes that his scores in the first few weeks of class will be reflected in the rest of the semester.\n",
    "\n",
    "For this problem you may make assumptions about what is known about Calculon's test results, i.e. his underlying test score variance.  Such assumptions should be explained in your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "If Calculon is a frequentist, what would he predict to be his final score in the course?  What would he say about the uncertainty of his score?  Calculate these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "If Calculon is a Bayesian, what would he predict his final score to be?  What is his confidence in this prediction?  Find these information numerically.  Since Calculon is a Bayesian in this example, he needs to choose a reasonable prior.  He notes that the average grade in the section from the previous year was a 60.  **Hint**: For the prior, think about how grades are designed to be distributed in classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "Use Bayesian methods to estimate the parameters of the spectral data from HW3.\n",
    "\n",
    "### Part A\n",
    "\n",
    "Write down, in $\\LaTeX$ preferably, the likelihood of obtaining a measurement $x_i$ from a model that is a mixture of two Gaussian distributions and a uniform distribution.  \n",
    "\n",
    "In some sense there are 7 parameters total: the amplitudes of the Gaussians and uniform distributions (3), and the mean and variance of the Gaussians (4).  However, the normalization condition ensures that the 3 amplitudes are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Again using $\\LaTeX$, write down a reasonable prior for the amplitudes of the Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "Load data generated from the a probability distribution with the shape of the emission line data from HW4.  This data is located at `line-sample.dat` from the class GitHub repository.\n",
    "\n",
    "Sample the posterior distribution (that you must determine from the previous parts of this HW) on a grid of possible values for the amplitudes of the components (two Gaussians plus a background)  For now, fix the other parameters (mu's and sigma's) to the values found in the previous homework via `scipy.curve_fit`.  What is the resulting point estimates and credible region for the amplitudes?  How do these estimates compare to the curve fit results from HW4?  \n",
    "\n",
    "Note that your estimates in *this* problem will not resemble the optimal parameters found in HW4 due to the fact that the curve in HW4 is not normalized.  You may wish to find a normalization factor to compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "For each of the Gaussian amplitudes, find the marginalized posterior distribution.  Do this by integrating over all other possible (non-fixed) parameter values, which can be performed by summing portions of the log posterior grid in part C.  Refer to Lecture 5.  Use the marginalized pdf to find new point estimates as the posterior means.  Compare to your previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E\n",
    "\n",
    "What would this problem be like if it had asked you to find point estimates for all of the distribution parameters in Part A?  What difficulties would emerge and why?  What if you had 100 model parameters?  1000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
