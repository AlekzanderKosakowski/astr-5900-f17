{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor=0.6\n",
    "\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'scroll': True,\n",
    "        'width': 1024*factor,\n",
    "        'height': 768*factor\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probability Distributions and Estimators\n",
    "\n",
    "Karen Leighly, 2017\n",
    "\n",
    "Resources for this material include:\n",
    " - Portions of Bishop 1.2\n",
    " - Ivezic Chapter 3\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's motivate probability distributions through the use of an example.\n",
    "\n",
    "Consider a fair coin.  If I flip that coin 20 times, how many times will I obtain \"heads\"?\n",
    "\n",
    "The naive view would say: 10 times.  Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Initiate python stuff\n",
    "%pylab inline\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "h=np.random.uniform(0.0,1.0,20)\n",
    "test=np.where(h >= 0.5)\n",
    "print len(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Repeated runs of this cell show that \"10\" is observed only some of the time.\n",
    "\n",
    "Let's look at this in more detail by running the cell multiple times, and counting the number of times the uniform distribution yields greater than equal to 0.5, i.e., \"heads\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Make a function that will count the number of times the draw from the uniform distributions is greater or equal to 0.5\n",
    "\n",
    "def one_trial(num_points):\n",
    "    h=np.random.uniform(0.0,1.0,20)\n",
    "    test=np.where(h >= 0.5)\n",
    "    num_heads=len(test[0])\n",
    "    return num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Call this function 1000 times, storing the results in a vector\n",
    "\n",
    "num_points=1000\n",
    "out=np.zeros(1000)\n",
    "\n",
    "for j in range(1000):\n",
    "    out[j]=one_trial(num_points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# and now plot\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15, 6)\n",
    "pylab.rcParams['axes.labelsize'] = 24\n",
    "\n",
    "pylab.rcParams['axes.titlesize'] = 24\n",
    "pylab.rcParams['lines.linewidth'] = 3\n",
    "pylab.rcParams['lines.markersize'] = 10\n",
    "pylab.rcParams['xtick.labelsize'] = 16\n",
    "pylab.rcParams['ytick.labelsize'] = 16\n",
    "\n",
    "plt.xlabel('Run',fontsize=20)\n",
    "plt.ylabel('Number of Heads',fontsize=20)\n",
    "\n",
    "\n",
    "plt.plot(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see, the result is not always 10 - there is a <i>distribution</i> around that value.\n",
    "\n",
    "Let us count how many points are in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_coin=np.arange(20)\n",
    "y_coin=np.zeros(20)\n",
    "for i in range(20):\n",
    "    temp=where(out == i)\n",
    "    y_coin[i]=len(temp[0])\n",
    "    \n",
    "plt.xlabel('Number of Heads',fontsize=20)\n",
    "plt.ylabel('Number of Times #Heads was drawn',fontsize=20)\n",
    "\n",
    "    \n",
    "plt.plot(x_coin+0.5,y_coin,drawstyle='steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This example yields some very useful concepts.\n",
    "\n",
    "- Even though we <i>expect</i> that there should be 10 \"heads\" per every 20 draws, there is actually a <i> distribution</i> of numbers of heads.  There can, in principle, be any value between zero and 20.\n",
    "\n",
    "- But the peak of the distribution is near 10, so it is consistent with the number that we expect. \n",
    "\n",
    "- At the same time, the distribution has some width, which gives us some information about how often we should expect very low or very high values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A probability distribution should be normalized\n",
    "\n",
    "Our histogram above is informative, but the $y$-axis values are related to the number of times the experiment has been performed, and that, of course, may change.\n",
    "\n",
    "So, if we want to talk about the probability of observing any particular value, we need to normalize the distribution by dividing by the total number of draws.  Let's do this.\n",
    "\n",
    "Note that the result should be referred to as a _probability density_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y_coin_normalized=y_coin/y_coin.sum()\n",
    "plt.xlabel('Number of Heads',fontsize=20)\n",
    "plt.ylabel('Probability Density',fontsize=20)\n",
    "\n",
    "\n",
    "plt.axis([0,20,0,y_coin_normalized.max()*1.1],fontsize=20,linewidth=4)\n",
    "\n",
    "\n",
    "plt.axis(fontsize=20,linewidth=4)\n",
    "\n",
    "plt.plot(x_coin+0.5,y_coin_normalized,drawstyle='steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# And here are the values for y_coin and y_coin_normalized.  We will come back to this later.\n",
    "\n",
    "for i in np.arange(20):\n",
    "    print x_coin[i],y_coin[i],y_coin_normalized[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binomial Distribution\n",
    "\n",
    "This is known as the binomial distribution.  \n",
    "\n",
    "The binomial distribution has two parameters: \n",
    " - $N$ is the number of trials (in our example, $N=20$), and \n",
    " - $b$, which is the probability of \"success\" for each draw (in our example, $b=0.5$).   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If the probability of a \"success\" is $b$, then what is the probability of a failure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The probability of a \"failure\" is $1-b$.  \n",
    "\n",
    "In our case, $b=0.5$ (a fair coin) but it can be a different number between 0 and 1 (a weighted coin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, then, the probability that $k$ successes occurred on any one set of trials is\n",
    "\n",
    "$$p(k|b,N)=\\frac{N!}{k!(N-k)!} b^k (1-b)^{N-k}$$\n",
    "\n",
    "where the \"!\" stands for factorial.\n",
    "\n",
    "The special case of $N=1$ is known as the _Bernoulli_ distribution.\n",
    "\n",
    "From our discussions on Monday, this can be understood as the probability of $k$ consecutive successes followed by $(N-k)$ consecutive failures multiplied by the number of different permultations of such a draw.  \n",
    "\n",
    "scipy.stats has an implementation of the biniomial distribution, and we can use the \"pmf\" method to examine the probability distribution from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dist=scipy.stats.binom(20,0.5)\n",
    "ypdf=np.zeros(20)\n",
    "print dist.pmf(8)\n",
    "for i in range(20):\n",
    "    ypdf[i]=dist.pmf(x_coin[i])\n",
    "\n",
    "plt.xlabel('Number of Heads',fontsize=20)\n",
    "plt.ylabel('Probability Density',fontsize=20)\n",
    "    \n",
    "plt.plot(x_coin+0.5,y_coin_normalized,drawstyle='steps')\n",
    "plt.plot(x_coin,ypdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see that the pmf method on the binom yields a good representation of the generated distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aside about methods\n",
    "\n",
    "_Methods_, in my experience, are a phenomenon unique to python that require a shift in thinking to use effectively.  However, the binom provides a good example of how methods work.\n",
    "\n",
    "First, we compute an \"instance\" of binom for the specific values of 20 and 0.5, as follows:\n",
    "\n",
    "dist=scipy.stats.binom(20,0.5)\n",
    "\n",
    "That instance, now called \"dist\", lives in the memory.\n",
    "\n",
    "But there are a lot of things you might want to obtain from the specific example.  For the binomial distribution, those are available as [scipy.stats.binom](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html)\n",
    "\n",
    "| Method | Notes |\n",
    "| ------------- |-------------|\n",
    "| rvs(n, p, loc=0, size=1, random_state=None) |\tRandom variates. |\n",
    "| pmf(k, n, p, loc=0)\t| Probability mass function.| \n",
    "| logpmf(k, n, p, loc=0)\t| Log of the probability mass function.| \n",
    "| cdf(k, n, p, loc=0)\t| Cumulative distribution function.| \n",
    "| logcdf(k, n, p, loc=0)\t| Log of the cumulative distribution function.| \n",
    "| sf(k, n, p, loc=0)| \tSurvival function (also defined as 1 - cdf, but sf is sometimes more accurate).| \n",
    "| logsf(k, n, p, loc=0)\t| Log of the survival function.| \n",
    "| ppf(q, n, p, loc=0)| \tPercent point function (inverse of cdf — percentiles).| \n",
    "| isf(q, n, p, loc=0)\t| Inverse survival function (inverse of sf).| \n",
    "| stats(n, p, loc=0, moments='mv')| \tMean(‘m’), variance(‘v’), skew(‘s’), and/or kurtosis(‘k’).| \n",
    "| entropy(n, p, loc=0)\t| (Differential) entropy of the RV.| \n",
    "| expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)\t| Expected value of a function (of one argument) with respect to the distribution.| \n",
    "| median(n, p, loc=0)| \tMedian of the distribution.| \n",
    "| mean(n, p, loc=0)| \tMean of the distribution.| \n",
    "| var(n, p, loc=0)| \tVariance of the distribution.| \n",
    "| std(n, p, loc=0)\t| Standard deviation of the distribution.| \n",
    "| interval(alpha, n, p, loc=0)\t| Endpoints of the range that contains alpha percent of the distribution| \n",
    "\n",
    "These now do not have to be computed separately; they are already part of the dist instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "We will want to describe our statistical distributions with convenient measures of the \"location\" and \"shape\" of them. \n",
    "\n",
    "We will differentiate betweeen:\n",
    " - population statistics - parameters based on the distributions themselves.\n",
    " - sample statistics - values computed from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some of these are related to expectation values:\n",
    "\n",
    "Specifically, the expectation of f(x) is\n",
    "\n",
    "$$\\mathbb{E}[f]=\\sum_x p(x)f(x)$$\n",
    "for the discrete case, and \n",
    "$$\\mathbb{E}[f]=\\int p(x)f(x)dx$$\n",
    "for the continuous case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, the expectation of $x$ yields the mean of the probability distribution $h(x)$.\n",
    "$$\\mu=\\int_{-\\infty}^{\\infty} x h(x)dx.$$\n",
    "\n",
    "The variance of $f(x)$ provides a measurement of how much variability there is in $f(x)$ around its mean value $\\mathbb{E}[f(x)]$.  It is defined as:\n",
    "\n",
    "$$var[f]=\\mathbb{E}[(h(x)-\\mathbb{E}[h(x)])^2]$$\n",
    "\n",
    "which can also be written as:\n",
    "\n",
    "$$\\sigma^2 = V = \\int_{-\\infty}^{\\infty}  (x-\\mu)^2 h(x) dx,$$\n",
    "\n",
    "i.e., the expectation of the square distance of points from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Other estimators can be defined that are useful in different circumstances.  These include\n",
    " - skewness - useful to characterize the asymmetry of a distribution\n",
    " \n",
    " $$\\Sigma = \\int_{-\\infty}^\\infty \\left(\\frac{x-\\mu}{\\sigma}\\right)^3 h(x) dx$$\n",
    " \n",
    " \n",
    " - kurtosis - useful to characterize the shape of a distribution (peaked or flat)\n",
    " \n",
    " $$K=\\int_{-\\infty}^\\infty \\left(\\frac{x-\\mu}{\\sigma}\\right)^4 h(x) dx-3$$ \n",
    " \n",
    "Example of skew and kurtosis.  You can see that in a sense, they are defined relative to the normal distribution (which we have not talked about yet).\n",
    "![Figure 3.6](http://www.astroml.org/_images/fig_kurtosis_skew_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " \n",
    " - absolute deviation - like the variance, but without the square (also called an L1 estimator).  Useful (in my experience) when outliers are present.\n",
    " $$\\delta=\\int_{-\\infty}^{\\infty} \\lvert x-d \\rvert h(x)dx$$\n",
    " - Mode (or the most probable value in the case of unimodal functions), $x_m$\n",
    " \n",
    "Mode is implemented as: mode = scipy.stats.mode(data)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "- p% percentiles - e.g., quartiles ($q_{25}$, median=$q_{50}$, $q_{75}$)\n",
    "\n",
    "The difference between the third  and forst quartiles is called the interquartile range.  \n",
    "\n",
    "It is argued that these are more robust estimators than the mean and standard deviation, by which I mean that they are less affected by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Back to the binomial distribution. Rcall that $b$ is the probability of success, and $N$ is the number of draws. \n",
    "\n",
    "For the binomial distribution:\n",
    " - the mean (expected number of successes) is $\\bar{k}=bN$\n",
    " - the standard deviation is $\\sigma_k=[Nb(1-b)]^{1/2}$\n",
    " \n",
    "Note that these are the _population_ statistics, i.e., expectation values given the distribution, $b$, and $N$.\n",
    "\n",
    "Examples of the binomial distribution are seen below.\n",
    "\n",
    "![Figure 3.6](http://www.astroml.org/_images/fig_binomial_distribution_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Multinomial Distribution\n",
    "\n",
    "The multinomial distribution is a generalization of the binomial distribution.  It describes the distribution of a variable that can have more than two discrete values (true or false, as above, which we dummied up by using $>0.5$ and $<0.5$ in our example), but rather $M$ discrete value, with the probability of each value being different.   \n",
    "\n",
    "Example: I once refereed a paper in which something was classified as either increasing, decreasing, or staying the same.  The authors wanted to make some claims based on the number that were increasing, if I remember correctly, so they wanted to determine what the probability that the number they observed could have been obtained by chance. This  was a job for a multinomial distribution, which could have been dummied up from a uniform distribution by looking at $<0.333$, between 0.3333 and 0.6666, and $>0.6666$.  \n",
    "\n",
    "The python distribution of multinomial is called [numpy.random.multinomial](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.multinomial.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uncertainty in the bin counts\n",
    "\n",
    "Back to our binomial histogram $y_{coin}$, plotted below.  Does anybody have an idea how to compute the uncertainty on the bins?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "plt.axis([0,20,0,y_coin.max()*1.1],fontsize=20,linewidth=4)\n",
    "\n",
    "\n",
    "plt.axis(fontsize=20,linewidth=4)\n",
    "\n",
    "plt.plot(x_coin+0.5,y_coin,drawstyle='steps')\n",
    "plt.plot(x_coin,y_coin,'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Poisson Distribution\n",
    "\n",
    "The Poission distribution is a distribution that is near and dear to the heart of X-ray astronomers.  It is a discrete probability distribution that expresses the probability that a certain number of events has occurred in a fixed interval of time (or space) given that the average rate of events is known, and the events are independent.\n",
    "\n",
    "So, it is related to the binomial distribution if the number of trials goes to infinity so that the probability of success, $p=k/N$ stays fixed, and thus the distribution of the number of successes, $k$, is controlled by $/mu=pN$\n",
    "\n",
    "$$p(k|\\mu)=\\frac{\\mu^k \\exp(-\\mu)}{k!}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The estimators for the poission distribution are:\n",
    " - mean: $\\mu$\n",
    " - standard deviation: $\\sqrt{\\mu}$\n",
    " - skewness: $1/\\sqrt{\\mu}$\n",
    " - kurtosis: $1/\\mu$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can estimate the uncertainty in our histogram above by taking the square root of the number of counts in each bin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y_coin_err=sqrt(y_coin)\n",
    "plt.xlabel('Number of Heads',fontsize=20)\n",
    "plt.ylabel('Number of Times #Heads was drawn',fontsize=20)\n",
    "plt.errorbar(x_coin,y_coin,yerr=y_coin_err,linestyle='None')\n",
    "plt.plot(x_coin,y_coin,'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will talk about histograms in much more detail when we discuss cluster analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability Densities\n",
    "\n",
    "Let us think more generally about what a probability density is and how it can be defined.  In the same time, let's also generalize to cases where the variable can be continuous rather than discrete (i.e., the height of a person, versus the number of heads).\n",
    "\n",
    "If the probability of a variable $x$ falling in an interval between $x$ and $x+\\delta x$ is given by $p(x)\\delta x$ as $\\delta x$ approaches $0$, then $p(x)$ is called the probability density over $x$.  \n",
    "\n",
    "For illustration, let's make up a probability density comprised of two Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dist1=scipy.stats.norm(0,1)\n",
    "dist2=scipy.stats.norm(2,0.5)\n",
    "x=-4.0+0.01*np.arange(800)\n",
    "y1=np.zeros(800)\n",
    "y2=np.zeros(800)\n",
    "\n",
    "for i in range(800):\n",
    "    y1[i]=dist1.pdf(x[i])\n",
    "    y2[i]=dist2.pdf(x[i])\n",
    "yout=y1*2+2*y2\n",
    "\n",
    "#Don't forget to normalize\n",
    "prob_dist=yout/scipy.integrate.trapz(yout,x)\n",
    "plt.ylabel('Probability Density',fontsize=20)\n",
    "plt.xlabel('Variable',fontsize=20)\n",
    "\n",
    "plt.plot(x,prob_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Properties of a Probability Density Function\n",
    "\n",
    "The probability density $p(x)$ must satisfy two conditions:\n",
    "$$p(x) \\ge 0$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty} p(x) dx = 1.$$\n",
    "\n",
    "We can see that the first is true from the plot, and we normalized the function (using `scipy.integrate.trapz`), so we the second is true.\n",
    "\n",
    "Also useful is the cumulative distribution function \n",
    "$$P(z)=\\int_{-\\infty}^z p(x) dx$$\n",
    "which we compute next.  \n",
    "\n",
    "Naturally, given the definition, the minimum value of the cumulative distribution function is 0, and the maximum value of the cumulative distribution function is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cumfunc=np.zeros(800)\n",
    "for i in range(800):\n",
    "    cumfunc[i]=scipy.integrate.trapz(prob_dist[0:i],x[0:i])\n",
    "\n",
    "plt.ylabel('Cumulative Probability Density',fontsize=20)\n",
    "plt.xlabel('Variable',fontsize=20)\n",
    "\n",
    "\n",
    "plt.plot(x,cumfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Let's compute mean, variance, and standard deviation for the distribution above, using the expectation value formalism, i.e., \n",
    "\n",
    "$$\\mu=\\int_{-\\infty}^{\\infty} x h(x)dx.$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\sigma^2 = V = \\int_{-\\infty}^{\\infty}  (x-\\mu)^2 h(x) dx,$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu=scipy.integrate.trapz(prob_dist*x,x)\n",
    "print mu\n",
    "\n",
    "var=scipy.integrate.trapz(((x-mu)**2)*prob_dist,x)\n",
    "print var\n",
    "\n",
    "sigma=np.sqrt(var)\n",
    "print sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Gaussian (normal) Distribution\n",
    "\n",
    "The normal distribution is commonly used, and even more commonly assumed, so let's discuss its properties in some detail.  \n",
    "\n",
    "We write the normal distribution of $x$ as: $\\mathcal{N}(x|\\mu,\\sigma^2)$.  \n",
    "\n",
    "This nomenclature refers to the fact that the distribution is characterized by the <i>mean</i> $\\mu$ and the <i>variance</i> $\\sigma^2$.  \n",
    " - The square root of the variance is the <i>standard deviation</i>, \n",
    " - while the inverse of the variance, $1/\\sigma^2$ is known as the <i>precision</i>.  (The latter is more useful than you might think, given that $\\sigma^2$ always appears in the denominator.)\n",
    "\n",
    "\n",
    "The normal (Gaussian) probability distribution is given by: \n",
    "\n",
    "$$p(x)=\\mathcal{N}(x|\\mu,\\sigma^2)=\\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right]$$\n",
    "\n",
    "where the $\\mathcal{N}$ generically refers to the Gaussian distribution, and $\\mu$ and $\\sigma$ are the mean and standard deviation, which we will discuss shortly.\n",
    "\n",
    "The Gaussian distribution for a few different values of $\\mu$ and $\\sigma$ can be seen below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dist1=scipy.stats.norm(0,1.5)\n",
    "dist2=scipy.stats.norm(1,1)\n",
    "dist3=scipy.stats.norm(-1,0.5)\n",
    "\n",
    "y1=np.zeros(800)\n",
    "y2=np.zeros(800)\n",
    "y3=np.zeros(800)\n",
    "\n",
    "for i in range(800):\n",
    "    y1[i]=dist1.pdf(x[i])\n",
    "    y2[i]=dist2.pdf(x[i])\n",
    "    y3[i]=dist3.pdf(x[i])\n",
    "\n",
    "plt.plot(x,y1)\n",
    "plt.plot(x,y2)\n",
    "plt.plot(x,y3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Computation of the mean and standard deviations will be left as an exercise.\n",
    "\n",
    "The cumulative distribution for a Gaussian \n",
    "\n",
    "$$P(x|\\mu,\\sigma) =\\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\int_{-\\infty}^x exp\\left[-\\frac{1}{2\\sigma^2}(x^\\prime-\\mu)^2\\right]dx^\\prime$$\n",
    "\n",
    "cannot be expressed in closed form, and it is expressed in terms of the Gauss error function:\n",
    "\n",
    "$$erf(z)=\\frac{2}{\\sqrt{\\pi}} \\int_0^z \\exp (-t^2)dt$$\n",
    "\n",
    "so that \n",
    "\n",
    "$$P(x|\\mu,\\sigma)=\\frac{1}{2} (1\\pm erf(\\frac{|x-\\mu|}{\\sigma \\sqrt{2}}))$$\n",
    "\n",
    "Computation of the cumulative function will be left as an exercise.  Note that erf is is available in python as [scipy.special.erf](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.special.erf.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Useful properties of the Gaussian\n",
    "\n",
    "The Fourier transform of a Gaussian is a gaussian.  This means that the convolution of a Gaussian distribution with another Gaussian distribution also yields a Gaussian distriubtion.  This is useful for Bayesian statistics, if, for example, your likelihood is based on $\\chi^2$, which has similar structure as a Gaussian, and your prior is also a Gaussian.\n",
    "\n",
    "So for example, if you have \n",
    " - $\\mathcal{N}(\\mu_0,\\sigma_0)$ (for example, the intrinsic distribution that you are trying to measure)\n",
    "and it is convolved with \n",
    " - $\\mathcal{N}(b,\\sigma_e)$ (for example, a error distribution with bias $b$ and random error $\\sigma_e$\n",
    "The result will have $\\mathcal{N}(\\mu_c,\\sigma_c)$, where\n",
    "$$\\mu_c=\\mu_0+b$$\n",
    "and\n",
    "$$\\sigma_c=(\\sigma_0^2+\\sigma_e^2)^{1/2}.$$\n",
    "\n",
    "There are other useful properties that we will touch on later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sample Statistics from a Gaussian\n",
    "\n",
    "\n",
    "Consider we have a sample of points $\\mathbf{X}=(x_1,x_2,\\dotsc,x_N)$ that we know to have been drawn from a Gaussian distribution whose mean and variance are unknown.  We would like to determine the mean and variance from the data set.  How do we do this?\n",
    "\n",
    "We can assume that these are _independently_ drawn.  Then the probability product rule tells us that the joint probability of two independent events is the product of the marginal probabilties for each event separately.  Thus, the probability of the data set is\n",
    "\n",
    "$$p(\\mathbf{X}|\\mu,\\sigma^2) = \\prod_{n=1}^{N} \\mathcal{N}(x_n|\\mu,\\sigma^2)$$\n",
    "\n",
    "This is the <i>likelihood function</i> for the Gaussian as a function of $\\mu$ and $\\sigma$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common criterion for determining the parameters in a probability distribution using an observed data set is to find the parameter values that maximize the likelihood function.  It is convenient to take the natural log of the likelihood function, however, before finding the parameters that maximmize it.  Since the natural log is a monotonic function, the parameters that maximize a function also maximize its natural log.  \n",
    "\n",
    "Then the log likelihood function is written as:\n",
    "\n",
    "$$\\ln p(\\mathbf{X} | \\mu,\\sigma^2) = -\\frac{1}{2\\sigma^2} \\sum_{n=1}^{N}(x_n-\\mu)^2 - \\frac{N}{2} \\ln \\sigma^2 - \\frac{N}{2} \\ln (2\\pi).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can maximize this function with respect to $\\mu$ and $\\sigma$ by taking the derivative with respect to these two variables and setting the result equal to zero.\n",
    "\n",
    "$$0=\\frac{\\partial}{\\partial\\mu} \\sum_{n=1}^{N} (x_n-\\mu)^2$$\n",
    "$$=\\sum_{n=1}^{N} \\frac{\\partial}{\\partial\\mu} (x_n-\\mu)^2$$\n",
    "$$=\\sum_{n=1}^{N} 2(x_n-\\mu)(-1)$$\n",
    "$$\\mu_{ML}=\\frac{1}{N} \\sum_{n-1}^N x_n$$\n",
    "\n",
    "This is the sample mean, i.e,. the mean of the observed values ${x_n}$.  This value for $\\mu$ maximizes the log-likelihood function, and is a very familiar function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is similarly easy to show determine the maximum likelihood estimate of $\\sigma^2$ by differentiating the above with respect to $\\sigma^2$ and setting equal to zero:\n",
    "\n",
    "$$0=\\frac{\\partial}{\\sigma^2}[-\\frac{1}{2\\sigma^2} \\sum_{n=1}^{N}(x_n-\\mu)^2 - \\frac{N}{2} \\ln \\sigma^2]$$\n",
    "\n",
    "$$=-\\frac{1}{2}\\sum_{n=1}^N(x_n-\\mu)^2 \\frac{\\partial}{\\sigma^2} \\big( \\frac{1}{\\sigma^2} \\big)- \\frac{N}{2} \\frac{\\partial}{\\sigma^2} \\ln \\sigma^2$$\n",
    "\n",
    "$$=-\\frac{1}{2}\\sum_{n=1}^N(x_n-\\mu)^2 \\big(\\frac{-1}{\\sigma^4}\\big) - \\frac{N}{2} \\frac{1}{\\sigma^2}$$\n",
    "\n",
    "$$\\sigma^2_{ML}=\\frac{1}{N} \\sum_{n=1}^N(x_n-\\mu_{ML})^2.$$\n",
    "\n",
    "So now you know where these familiar formulae come from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gaussian confidence levels\n",
    "\n",
    "The probability of a measurement drawn from a Gaussian distribution that is between $\\mu-a$ and $\\mu+b$ is\n",
    "$$\\int_{\\mu-a}^{\\mu+b} p(x|\\mu,\\sigma) dx.$$\n",
    "For $a=b=1\\sigma$, we get the familar result of 68.3%.  For $a=b=2\\sigma$ it is 95.4%.  So we refer to the range $\\mu \\pm 1\\sigma$ and $\\mu \\pm 2\\sigma$ as the 68% and 95% **confidence limits**, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalized Normal Distribution\n",
    "\n",
    "The _generalized normal distribution_ can sometimes come in handy. For a location $\\mu$, a scale parameter $\\alpha$, and a shape parameter $\\beta$, the probability distribution function is:\n",
    "\n",
    "$$h(x)=\\frac{\\beta}{2\\alpha \\Gamma(1/\\beta)} e^{-(\\lvert x-\\mu \\rvert / \\alpha)^\\beta}$$\n",
    "\n",
    "where $\\gamma$ is the gamma function.\n",
    "\n",
    "- The case where $\\beta=1$ is known as the Laplace distribution.\n",
    "- The case where $\\beta=2$ is the Gaussian distribution.\n",
    "\n",
    "Plots of the generalized normal distribution are seen here.:![generalized normal distribution](https://upload.wikimedia.org/wikipedia/commons/1/10/Generalized_normal_densities.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Central Limit Theorem\n",
    "\n",
    "Why is the Gaussian distribution so popular and commonly used (and, importantly, assumed)?  First, it has a number of convenient properties.\n",
    " - the convolution of two Gaussian functions is also a Gaussian.\n",
    " - the Fourier transform of a Gaussian is also a Gaussian.\n",
    " \n",
    "But the most important property is the Central Limit Theorem, which states \"Given an arbitrary distribution $h(x)$, characterized by a mean $\\mu$ and standard deviation $\\sigma$, the mean of $N$ values $x_i$ drawn from that distribution will approximately follow a Gaussian distribution $\\mathcal{N}(\\mu,\\sigma/\\sqrt{N}$), with the approximation accuracy improving with $N$.\"\n",
    "\n",
    "This theorem is the foudation for the performing repeat measurements in order to improve the accuracy of one's experiment.  It is telling us something about the *shape* of the distribution that we get when averaging.  The **Law of Large Numbers** further says that the sample mean will converge to the distribution mean as $N$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us explore this concept by experiment.  Consider 10000 numbers drawn from a uniform distribution, and create a histogram of those values between zero and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x=np.random.uniform(0,1,10000)\n",
    "print x.shape\n",
    "plt.hist(x,bins=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It doesn't look much like a normal distribution (in fact, it looks like a uniform distribution), but we have only one draw per trial.\n",
    "\n",
    "Now, what if at every trial, you picked two, and recorded the average value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x=np.random.uniform(0,1,20000)\n",
    "out=np.zeros(10000)\n",
    "for i in range(10000):\n",
    "    out[i]=(x[2*i]+x[2*i+1])/2\n",
    "\n",
    "plt.hist(out,bins=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now the distribution is peaked in the middle.  Does this make sense?\n",
    "\n",
    "\n",
    "How about 10 draws per data point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x=np.random.uniform(0,1,100000)\n",
    "out=np.zeros(10000)\n",
    "temp=np.zeros(10)\n",
    "for i in range(10000):\n",
    "    temp[0:10]=x[10*i:10*(i+1)]\n",
    "    out[i]=temp.mean()\n",
    "    \n",
    "plt.hist(out,bins=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see, we have generated a reasonable bell curve from the uniform distribution by simply drawing a large number of trials and averaging.\n",
    "\n",
    "## Homework: \n",
    "You will have the opportunity to explore the central limit theorem for the Poission distribution in the HW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Cauchy (Lorenzian) Distribution\n",
    "\n",
    "The Cauchy distribution is defined in terms of location parameter $\\mu$ and shape parameter $\\gamma$ as\n",
    "\n",
    "$$p(x|/mu,\\gamma)= \\frac{1}{\\pi \\gamma} \\left(\\frac{\\gamma^2}{\\gamma^2+(x-\\mu)^2}\\right).$$\n",
    "\n",
    "Examples of the Cauchy distribution are seen in Fig. 3.11 of Ivezic:\n",
    "\n",
    "![Figure 3.11](http://www.astroml.org/_images/fig_cauchy_distribution_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A notable feature of this distribution is the long tails.  It turns out that for the Cauchy distribution, the median and mode are both equal to $\\mu$, but the _mean, variance, standard deviation, and higher moments do not exist_.  \n",
    "\n",
    "Basically, the long tails mean that there is finite probability that large values will be drawn from the distribution that would skew any estimates of the mean, etc.  In other words, one can compute a mean $\\mu$ from data drawn from the Cauchy distribution, but it would have large scatter around the value, and this scatter _will not decrease as the sample size increases_.  \n",
    "\n",
    "What this means is that the _Central Limit theorem will not hold for data drawn from a Cauchy distribution_, because the Central Limit theorem only holds for distributions that have actual means and standard deviations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's investigate this assertion by setting setting up some instances of gaussian and normal distributions, and computing the mean and standard deviation for increasing numbers of draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "normal_dist=scipy.stats.norm(0,1)\n",
    "cauchy_dist=scipy.stats.cauchy(0,1)\n",
    "\n",
    "num_draws=2+arange(1000)\n",
    "norm_dist_mean=np.zeros(1000)\n",
    "norm_dist_sigma=np.zeros(1000)\n",
    "cauchy_dist_mean=np.zeros(1000)\n",
    "cauchy_dist_sigma=np.zeros(1000)\n",
    "for i in range(1000):\n",
    "    temp_normal=normal_dist.rvs(num_draws[i])\n",
    "    temp_cauchy=cauchy_dist.rvs(num_draws[i])\n",
    "    norm_dist_mean[i]=np.mean(temp_normal)\n",
    "    norm_dist_sigma[i]=np.std(temp_normal)\n",
    "    cauchy_dist_mean[i]=np.mean(temp_cauchy)\n",
    "    cauchy_dist_sigma[i]=np.std(temp_cauchy)\n",
    "    \n",
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "plt.axis([0,1002,-1,1],fontsize=20,linewidth=4)\n",
    "plt.ylabel('Normal Distribution Mean',fontsize=20)\n",
    "plt.xlabel('Number of Draws',fontsize=20)\n",
    "\n",
    "plt.plot(num_draws,norm_dist_mean)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.axis([0,1002,-200,200],fontsize=20,linewidth=4)\n",
    "plt.ylabel('Cauchy Distribution Mean',fontsize=20)\n",
    "plt.xlabel('Number of Draws',fontsize=20)\n",
    "plt.plot(num_draws,cauchy_dist_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "plt.axis([0,1002,0,2],fontsize=20,linewidth=4)\n",
    "plt.ylabel('Normal Distribution Sigma',fontsize=20)\n",
    "plt.xlabel('Number of Draws',fontsize=20)\n",
    "\n",
    "plt.plot(num_draws,norm_dist_sigma)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.axis([0,1002,0,200],fontsize=20,linewidth=4)\n",
    "plt.ylabel('Cauchy Distribution Sigma',fontsize=20)\n",
    "plt.xlabel('Number of Draws',fontsize=20)\n",
    "plt.plot(num_draws,cauchy_dist_sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It turns out that the ratio of two independent standard normal variables ($z=(x-\\mu)/\\sigma)$, with $z$ drawn from $\\mathcal{N}(0,1)$ follows a Cauchy distribution with $\\mu=0$ and $\\gamma=1$.\n",
    "\n",
    "What this means is, to quote Ivezic \"Therefore, in cases where the quantity of interest is obtained as a ratio of two other measured quantities, assuming that it is distributed as a Gaussian is a really bad idea if the quantity in the deonomianor has a finite chance of taking on a zero value.  Futhermore, using the mean value to determine its location parameter (i.e., to get a \"best\" value implied by the measurements) will not achieve the 1/\\sqrt{N}$ error reduction.\n",
    "\n",
    "They also note that the ratio of two general Gaussians will follow a Hinkley distribution.\n",
    "\n",
    "**The bottom line is that it is good to be cautious when looking at ratios.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Bivariate and Multivariate Gaussian Distributions\n",
    "\n",
    "The multivariate Gaussian distribution illustrates some properties that will be generally useful, in particular the concept of **covarience**.\n",
    "\n",
    "The $D$ dimensional vector of variable $\\mathbf{x}$, the normal distribution looks like this:\n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}, \\mathbf{\\Sigma})=\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}\\exp\\big[{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{\\mu}})\\big].$$\n",
    "\n",
    "Here, note the boldface: $\\mathbf{\\mu}$ is the D-dimensional mean vector, $\\Sigma$ is the $D\\times D$ covariance matrix, and $|\\mathbf{\\Sigma}|$ is the determinant of $\\mathbf{\\Sigma}$.  \n",
    "\n",
    "Note that bold-face indicates a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us consider some important properties of this function.  The value $\\Delta$ is a type of distance from $\\mathbf{x}$ to $\\mathbf{\\mu}$.    \n",
    "\n",
    "$$\\Delta^2={(\\mathbf{x}-\\mathbf{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{\\mu}})$$\n",
    "\n",
    "Note that this reduces to the Euclidean distance when $\\mathbf{\\Sigma}$ is the identity matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's explore some properties of $\\mathbf{\\Sigma}$.  \n",
    " - It must be symmetric, because any antisymmetric component would disappear from the exponent.\n",
    " - It is quadratic, and therefore lines of constant probability look like an ellipse (or generalization of an ellipse).  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us consider an example to illustrate some other properties.  Let $\\mathbf{\\mu}=[3,2]$, and $\\mathbf{\\Sigma}$ equal to\n",
    "\n",
    "\\begin{pmatrix}\n",
    "1.5 & 1 \\\\\n",
    "1 & 2 \\\\\n",
    "\\end{pmatrix}\n",
    "\n",
    "Let us sample this bivariate gaussian and plot it several different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (6, 6)\n",
    "\n",
    "mean = [3,2]\n",
    "cov = [[1.5,1], [1,2]]  \n",
    "x, y = np.random.multivariate_normal(mean, cov, 50000).T\n",
    "plt.plot(x, y, '.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So $x$ and $y$ together are distributed as a bivariate normal distribution.\n",
    "\n",
    "Let us first reproduce the mean and covariance from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print 'The mean of x is ',x.mean()\n",
    "print 'The mean of y is ',y.mean()\n",
    "mu_x=x.mean()\n",
    "mu_y=y.mean()\n",
    "x_center=x-mu_x\n",
    "y_center=y-mu_y\n",
    "print 'sigma_xx is ',(x_center*x_center).mean()\n",
    "print 'sigma_yy is ',(y_center*y_center).mean()\n",
    "print 'sigma_xy is ',(x_center*y_center).mean()\n",
    "\n",
    "print 'Compare with the covariance matrix ',cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "We can also plot as a 2-d histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist2d(x,y,bins=50,cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An important aspect of this result is that the two variables are correlated with one another and are therefore not independent.  This means that the distribution and associated estimators of each variable <i>separately</i> does not capture the probability density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's illustrate with an example.\n",
    "\n",
    "Imagine that you are presented with $\\mathbf{x}$ and $\\mathbf{y}$, two properties measured from some sample (e.g., redshift and luminosity of some quasars).\n",
    "\n",
    "Then imagine that you wanted to determine the probability that you would observe a quasar with $[x_1,y_1]$.  How might you do that?  \n",
    "\n",
    "Naively, you might be tempted to model each of  $\\mathbf{x}$ and $\\mathbf{y}$ separately as a Gaussian distribution, and <it> assuming that they are independent</it> multiply the probabilities of the two together to get the joint probability, as follows.  To illustrate, consider computing the probability density at $(3.5,2.5)$ and $(3.5,1.5)$.  This pair of points is symmetric around the $y$ distribution, but not symmetric on the joint distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(x,y,bins=50,cmap='Greys')\n",
    "\n",
    "xtemp=[3.5,3.5]\n",
    "ytemp=[2.5,1.5]\n",
    "\n",
    "plt.plot(xtemp,ytemp,'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(x,bins=50,histtype=u'step')\n",
    "plt.hist(y,bins=50,histtype=u'step')\n",
    "print x.mean(),x.std()\n",
    "print y.mean(),y.std()\n",
    "from scipy.stats import norm\n",
    "xdist=norm(x.mean(),x.std())\n",
    "ydist=norm(y.mean(),y.std())\n",
    "print 'The probability of x=3.5:', xdist.pdf(3.5)\n",
    "print 'The probability of y=2.5:', ydist.pdf(2.5)\n",
    "print 'The joint probability of x=3.5 and y=2.5, assuming that they are independent:',xdist.pdf(3.5)*ydist.pdf(2.5)\n",
    "print ''\n",
    "print 'The probability of x=3.5:', xdist.pdf(3.5)\n",
    "print 'The probability of y=1.5:', ydist.pdf(1.5)\n",
    "print 'The joint probability of x=3.5 and y=1.5, assuming that they are independent:',xdist.pdf(3.5)*ydist.pdf(1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In fact, you will need to consider the multivariate normal distribution, which accounts for the coupling between the two parameters, as follows.\n",
    "\n",
    " - First I compute the instance of the multivariate normal distribution with the mean and covariance given above.\n",
    " - Then I compute the probability density at our two example positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dist=scipy.stats.multivariate_normal(mean, cov)\n",
    "print dist.pdf([3.5,2.5])\n",
    "print dist.pdf([3.5,1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These are not equal to the naive estimated joint probability above.  This is probably not surprising since our computation above would predict that the probability should be the same at $(x-\\mu_x,y-\\mu_y)$ as at $(x+\\mu_x,y-\\mu_y)$, by the symmetry of the individual gaussians - but we can see from the contour plots that it is not.\n",
    "\n",
    "\n",
    "Are there any circumstances when the joint probability distribution (i.e., the product of the probabilities for the two distributions separately) is equal to the actual probability distribution?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The conditions under which the probability of the 2-dimensional Gaussian equals the joint probability of the Gaussian distributions of their projects is _when the covariance matrix is diagonal_.  \n",
    "\n",
    "How can we get a diagonal covariance matrix?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we rotate the coordinate system along the axes of symmetry, we will obtain a diagonal covariance matrix.\n",
    "\n",
    "How do we rotate the coordinate system? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we solve the eigenvector problem and use the matrix formed by the eigvenvectors to rotate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can show that this procedure will result in a joint probability density equal to the product of the individual probability densities, as follows:\n",
    "\n",
    "The general eigenvector equation for the D-dimensional covariance matrix:\n",
    "   $$\\mathbf{\\Sigma}\\mathbf{u}_i = \\lambda_i\\mathbf{u}_i$$\n",
    "   \n",
    "where $i=1,\\dots,D$.  \n",
    "\n",
    "Linear algebra tells us that since $\\mathbf{\\Sigma}$ is a real, symmetric matrix, its eigenvalues are real and the eigenvectors will form an orthonormal set, i.e., \n",
    "\n",
    "$$\\mathbf{u}_i^T \\mathbf{u}_j = I_{ij}$$\n",
    "where $\\mathbf{I}$ is the D-dimensional identity matrix.\n",
    "\n",
    "More results from linear algebra (see Bishop, section 2.3) yield\n",
    "\n",
    "$$\\mathbf{\\Sigma}^{-1} = \\sum_{i=1}^D \\frac{1}{\\lambda_i} \\mathbf{u}_i \\mathbf{u}_i^T, $$\n",
    "\n",
    "with the quadratic form in the Gaussian function becoming:\n",
    "\n",
    "$$\\Delta^2 = \\sum_{i=1}^D \\frac{y_i^2}{\\lambda_i}$$\n",
    "\n",
    "where $y_i$ is defined as:\n",
    "\n",
    "$$y_i=\\mathbf{u}_i^T (\\mathbf{x}-\\mathbf{\\mu}).$$\n",
    "\n",
    "This is written more generally as:\n",
    "\n",
    "$$\\mathbf{y}=\\mathbf{U}(\\mathbf{x}-\\mathbf{\\mu})$$\n",
    "where $\\mathbf{U}$ is a matrix whose rows are given by $u_i^T$, and {$y_i$} is a shifted, rotated, and scaled coordinate system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What has been accomplished by this transformation to the $\\mathbf{y}$ coordinate system?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Assuming all eigenvectors are non-zero, in this coordinate system, the general equation for the 2-dimensional Gaussian simplifies dramatically, chiefly because the analog of the covariance matrix is now a diagonal matrix of eigenvalues.\n",
    "\n",
    "In addition, the determinant of the diagonal matrix is a product of the square root of the eigenvalues:\n",
    "$$|\\mathbf{\\Sigma}|^{1/2} = \\prod_{j=1}^D \\lambda_j^{1/2}.$$\n",
    "\n",
    "So, in the $y_i$ coordinate system, the Gaussian distribution takes the form:\n",
    "\n",
    "$$p(\\mathbf{y}) = \\prod_{j=1}^D \\frac{1}{(2 \\pi \\lambda_{j})^{1/2}} \\exp \\big[-\\frac{y_j^2}{2 \\lambda_j}\\big],$$\n",
    "i.e., a product of $D$ <i>independent</i> one dimensional Gaussian distributions.  So it has the property that we were looking for: the joint probability distribution factorizes into a product of independent distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Homework:\n",
    "\n",
    "Your homework has a problem that allows you to explore this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample vs. Population Statistics \n",
    "\n",
    "Statistics estimated from the *data* are called _sample statistics_ as compared to _population statistics_ which come from knowing the functional form of the pdf.  Up to now we have been computing population statistics.\n",
    "\n",
    "Specifically, $\\mu$ is the *population average*, i.e., it is the expecation value of $x$ for $h(x)$.  But we don't *know* $h(x)$.  So the **sample mean**, $\\overline{x}$, is an *estimator* of $\\mu$, defined as\n",
    "$$\\overline{x} \\equiv \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "which we determine from the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then instead of $\\sigma^2$, which is the population variance, we have the **sample variance**, $s^2$, where\n",
    "\n",
    "$$s^2 = \\frac{1}{N-1}\\sum_{i=1}^N(x_i-\\overline{x})^2$$\n",
    "\n",
    "Where it is $N-1$ instead of $N$ since we had to determine $\\overline{x}$ from the data instead of using a known $\\mu$.  Ideally one tries to work in a regime where $N$ is large enough that we can be lazy and ignore this. \n",
    "\n",
    "So the mean and variance of a distribution are $\\mu$ and $\\sigma^2$.  The *estimators* of the distribution are $\\overline{x}$ (or $\\hat{x}$) and $s^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias\n",
    "\n",
    "If there is a difference between the *estimator* and the *population* values, we say that the estimator is **biased** (perhaps not quite the usage of the word that you are used to).  Again, more on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Uncertainty\n",
    "\n",
    "We would also like to know the uncertainty of our estimates $\\overline{x}$ and $s$.  Note that $s$ is **NOT** the uncertainty of $\\overline{x}$.  Rather the uncertainty of $\\overline{x}$, $\\sigma_{\\overline{x}}$ is \n",
    "$$ \\sigma_{\\overline{x}} = \\frac{s}{\\sqrt{N}},$$\n",
    "which we call the *standard error of the mean*.\n",
    "\n",
    "The uncertainty of $s$ itself is\n",
    "$$\\sigma_s = \\frac{s}{\\sqrt{2(N-1)}} = \\frac{1}{\\sqrt{2}}\\sqrt{\\frac{N}{N-1}}\\sigma_{\\overline{x}}.$$\n",
    "\n",
    "Note that for large $N$, $\\sigma_{\\overline{x}} \\sim \\sqrt{2}\\sigma_s$ and for small $N$, $\\sigma_s$ is not much smaller than $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Few More Distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "The uniform distribution is perhaps more commonly called a \"top-hat\" or a \"box\" distribution.  It is specified by a mean, $\\mu$, and a width, $W$, where\n",
    "\n",
    "$$p(x|\\mu,W) = \\frac{1}{W}$$\n",
    "\n",
    "over the range $|x-\\mu|\\le \\frac{W}{2}$ and $0$ otherwise.  That says that \"given $\\mu$ AND $W$, the probability of $x$ is $\\frac{1}{W}$\" (as long as we are within a certain range).\n",
    "\n",
    "Since we are used to thinking of a Gaussian as the *only* type of distribution the concept of $\\sigma$ (aside from the width) may seem strange.  But $\\sigma$ as mathematically defined above applies here and\n",
    "$$\\sigma = \\frac{W}{\\sqrt{12}}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Log Normal\n",
    "\n",
    "Note that if $x$ is Gaussian distributed with $\\mathscr{N}(\\mu,\\sigma)$, then $y=\\exp(x)$ will have a **log-normal** distribution, where the mean of y is $\\exp(\\mu + \\sigma^2/2)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $\\chi^2$ Distribution\n",
    "\n",
    "We'll run into the $\\chi^2$ distribution when we talk about Maximum Likelihood in the next lecture.\n",
    "\n",
    "If we have a Gaussian distribution with values ${x_i}$ and we scale and normalize them according to\n",
    "$$z_i = \\frac{x_i-\\mu}{\\sigma},$$\n",
    "then the sum of squares, $Q$ \n",
    "$$Q = \\sum_{i=1}^N z_i^2,$$\n",
    "will follow the $\\chi^2$ distribution.  The *number of degrees of freedom*, $k$ is given by the number of data points, $N$ (minus any constraints).  The pdf of $Q$ given $k$ defines $\\chi^2$ and is given by\n",
    "$$p(Q|k)\\equiv \\chi^2(Q|k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}Q^{k/2-1}\\exp(-Q/2),$$\n",
    "where $Q>0$ and the $\\Gamma$ function would just be the usual factorial function if we were dealing with integers, but here we have half integers.\n",
    "\n",
    "This is ugly, but it is really just a formula like anything else.  Note that the shape of the distribution *only* depends on the sample size $N=k$ and not on $\\mu$ or $\\sigma$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Student's $t$ Distribution\n",
    "\n",
    "Another distribution that we'll see later is the Student's $t$ Distribution.\n",
    "\n",
    "If you have a sample of $N$ measurements, $\\{x_i\\}$, drawn from a Gaussian distribution, $\\mathscr{N}(\\mu,\\sigma)$, and you apply the transform\n",
    "$$t = \\frac{\\overline{x}-\\mu}{s/\\sqrt{N}},$$\n",
    "then $t$ will be distributed according to Student's $t$ with the following pdf (for $k$ degrees of freedom): \n",
    "$$p(x|k) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{\\pi k} \\Gamma(\\frac{k}{2})} \\left(1+\\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}$$\n",
    "\n",
    "As with a Gaussian, Student's $t$ is bell shaped, but has \"heavier\" tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's the point?\n",
    "\n",
    "The point is that we are going to make some measurement.  And we will want to know how likely it is that we would get that measurement in our experiment as compared to random chance.  To determine that we need to know the shape of the distribution.  Let's say that we find that $x=6$.  If our data is $\\chi^2$ distributed with 2 degrees of freedom, then we would integrate the $k=2$ curve above from 6 to $\\infty$ to determine how likely it is that we would have gotten 6 or larger by chance.  If our distribution was instead $t$ distributed, we would get a *very* different answer.  Note that it is important that you decide *ahead of time* what the metric will be for deciding whether this result is significant or not.  More on this later, but see [this article](http://fivethirtyeight.com/features/science-isnt-broken/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
